<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shuai Shao</title>
    <description>单纯明快</description>
    <link>http://localhost:4000//</link>
    <atom:link href="http://localhost:4000//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Wed, 15 Jan 2020 21:35:00 -0800</pubDate>
    <lastBuildDate>Wed, 15 Jan 2020 21:35:00 -0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>An Overview of Interpretable Machine Learning</title>
        <description>&lt;p&gt;Definition of interpretability ([1], [2])
Though there is little consensus on a formal definition of  interpretability in machine learning, one good definition is that interpretability is the ability to explain or to present in understandable terms  to a human.&lt;/p&gt;

&lt;p&gt;Why interpretability?
Model interpretability is used to confirm below desiderata of ML systems:
Fairness: protected groups are not discriminated against
Privacy: the method protects sensitive information in the data
Reliability: whether algorithms is robust under parameter or input variation
Causality: the predicted change in output due to a perturbation will occur in real system
Trust: confidence from human users&lt;/p&gt;

&lt;p&gt;-Regulations like GDPR and CCPA require algorithms that make decisions based on user-level predictions, which “significantly affect” users to provide explanation (“right to explanation”).&lt;/p&gt;

&lt;p&gt;Scope of interpretation
Algorithm transparency: how does the algorithm create the model (focus on the knowledge of the algorithm, not the data or learned model)
Global model interpretability: comprehend the model at once, difficult to achieve due to the limitation of humans (imagine visualizing  a 5-dimensional hyperplane)
Interpret model prediction on a single instance&lt;/p&gt;

&lt;p&gt;Properties of good explanations
Contrastive:
Selective:
Social:
Focus on abnormal:
Truthful:
Consistent with prior beliefs:
General and portable:&lt;/p&gt;

&lt;p&gt;Evaluation Methods ([2]):
Application level evaluation: real humans, real tasks.
Human level evaluation: real humans, simplified tasks
Function level evaluation: no humans, proxy tasks&lt;/p&gt;

&lt;p&gt;Taxonomy of interpretation methods:
Feature summary statistics, feature summary visualization, model internals, data point, intrinsically interpretable model&lt;/p&gt;

&lt;p&gt;Interpretable Models
Linear models:
Easy to interpret when the number of features is limited (feature selection methods like LASSO can be used to restrict the number of features)
Interpretation:
Increasing the numerical feature by one unit changes the estimated outcome by its weight (numerical feature)
Changing the feature from the reference category to the other category changes the estimated outcome by the feature’s weight (categorical feature)
The importance of a feature in a linear regression model can be measured by the absolute value of its t-statistic
Visualization methods include weight plot, effect plot
For logistic regression, increasing the numerical feature by one unit changes the logarithm of the odds by its weight
Similar interpretations apply to other GLM or GAM models
Pros: universal, well-studied and accepted, backed up with statistical theories (confidence intervals, significant tests etc.)
Cons: assumptions on data generation process may not be valid, model performance sometimes often less than optimal, number of ways to transform model and features could be overwhelming&lt;/p&gt;

&lt;p&gt;Decision trees:
Simple interpretation: starting from the root node of the tree, traverse till the leaf node, obtain a path connecting conditions explaining why the prediction was made
The overall importance of a feature is the summed reduction of Gini index
Pros: capturing interactions between features, has a natural visualization, create human-friendly explanations, no need to transform features
Cons: fails to deal with linear relationships, which goes hand in hand with the lack of smoothness; unstable (a few changes in the training dataset can create a completely different tree); only interpretable when they are short (the number of terminal nodes increases quickly with depth)&lt;/p&gt;

&lt;p&gt;RuleFit ([7]):
Learns sparse linear models which include automatically detected interaction effects in the form of decision rules
First train an ensemble of trees (e.g random forest) based on the task of predicting the outcome of interest; Discard the prediction result but keep the decision rules decomposed from the trained trees, and train a sparse linear regression model (e.g LASSO)  using the original features and the additional features from the decision trees
Pros: automatically adds feature interaction to the linear models, good interpretable when rules are not too complicated (e.g maximum depth of the trees less than 3)
Cons: interpretability degrades with the increasing number of features, the performance of the model may not be optimal&lt;/p&gt;

&lt;p&gt;Model-agnostic Methods
Flexible, same method can be used for any type of underlying models.&lt;/p&gt;

&lt;p&gt;Partial Dependence Plot (PDP)
A global method: The method considers all instances and gives a statement about the global relationship of a feature with the predicted outcome.
Shows the marginal effect one or two features have on the predicted outcome of an ML model, estimated by calculating averages in the training data
Assumes no correlation between the feature of interest and the rest of the features
Pros: intuitive, clear interpretation, easy to implement, has a causal interpretation
Cons: maximum number of features in a PDP is two (due to human perception limitation), assumption of independence can be problematic, not revealing heterogeneous effects&lt;/p&gt;

&lt;p&gt;Individual Conditional Expectation (ICE)
One line per instance that shows how the prediction for the instance changes when a feature changes
PDP is the average of the lines in ICE; the two can be combined together
Pros: even more understandable than PDP, can uncover heterogeneous effects
Cons: same cons as PDP, plus can be overcrowded&lt;/p&gt;

&lt;p&gt;Accumulated Local Effects (ALE) ([9])
A fast and unbiased alternative to PDP, which describes how features influence the predictions of an ML model on average
Addresses the feature interaction problem with PDP and ICE
Calculates the differences in predictions instead of averages based on the conditional distribution of the features (instead of the marginal distribution in PDP and ICE)
ALE is preferred over PDP as a rule of thumb
Pros: unbiased (still works when features are correlated), faster to compute than PDPs, clear interpretation
Cons: can be a bit shaky based on the selection of intervals, not accompanied by ICE&lt;/p&gt;

&lt;p&gt;Feature Interaction ([7])
When features interact, the prediction cannot be expressed as the sum of the feature effect. One way to estimate the interaction strength is through Friedman’s H-statistic, which is the difference between the observed partial dependence function and the decomposed one without interactions. Friedman and Popescu also proposed a test statistic to evaluate whether the H-statistic differs significantly from zero.
Pros: backed up by underlying theory, possible to analyze higher order interactions, dimensionless and between 0 and 1
Cons: computationally expensive, the statistic is not yet available in a model-agnostic version&lt;/p&gt;

&lt;p&gt;Feature Importance
Permutation importance: increase in the prediction error after we permute the feature’s values
A feature is ‘important’ if shuffling its values increases the model error, because in this case the model relied on the feature for the prediction
When two features interact with each other, the importance of the interaction is included in the importance measurement of both features, the sum is larger than the drop in performance.
Not clear whether should compute importance on training or test data
Pros: easy interpretation, highly compressed, global insight, does not require retraining the model
Cons: unclear whether compute on training or test data, need access to the true outcome, depends on shuffling the feature which adds randomness, biased when features interact, adding a correlated feature can decrease the importance of the associated feature by splitting the importance between both features&lt;/p&gt;

&lt;p&gt;Global Surrogate
An interpretable model (linear model, decision tree etc.) trained to approximate the predictions of a black box model. The labels are the output of the black box model instead of the true labels.
Not too useful. There’s a trade-off between approximation accuracy and interpretability of the surrogate model.
Pros: flexible, intuitive, easy to measure performance by R-square
Cons: no clear cut for R-square, could close to the black box model on one dataset but diverge on another&lt;/p&gt;

&lt;p&gt;Local Surrogate (LIME) ([10])
Interpretable models that are used to explain individual predictions of black box ML models
Local interpretable model-agnostic explanations (LIME) generates a new dataset consisting of perturbed samples and the corresponding predictions of the black box model. On this new dataset LIME then trains an interpretable model, which is weighted by the proximity of the sampled instances to the instance of interest.
The learned model should be a good approximation of the machine learning model predictions locally, but it does not have to be a good global approximation.
Different methods are used to generate  the perturbed samples for different data formats: superpixels for images, randomly removing words for texts etc.
Pros: explanations are short and contrastive, works for tabular data, images, and texts, have a good fidelity measure
Cons: a correct definition of neighbors remains unresolved for tabular data, current sampling method ignores feature interaction, instability
Note: the method is promising but is still in development, where research opportunities lie&lt;/p&gt;

&lt;p&gt;Integrated Gradients ([4])
Integration of the gradients of predictions with respect to features, along the linear interpolation path between the instance of interest and a baseline instance.
Satisfies two desirable characteristics (axioms) for feature attribution methods: sensitivity and implementation invariance; also proved that previous methods DeepLift, Layer-wise relevance propagation (LRP), de-convolutional network, and guided-back-propagation violated one axiom or the other.
Fast and scalable, only requires a few calls to the gradient operator (compared to the Shapley Values)&lt;/p&gt;

&lt;p&gt;Shapley Values
A method from coalitional game theory, which describes how to fairly distribution the prediction among the features
The Shapley value is the average marginal distribution of a feature value across all possible coalitions, which is a computation of feature contributions for single predictions for any machine learning model
The only attribution method that satisfies efficiency, symmetry, dummy, and additivity, which together can be the definition of a fair payout
Shapley values might be the only method to deliver a full explanation. In situations where law requires explainability - like GDPR’s ‘right to explanations’,  the Shapley values might be the only legally compliant method, since it’s based on a solid theory, and distributes the effects fairly
Very expensive to compute, usually through Monte-Carlo sampling
Pros: the difference between the prediction and the average prediction is fairly distributed among the features of the instance, allows contrastive explanation, backed up by a solid theory
Cons: computationally expensive, sampling may increase variance, use all features thus not sparse, no prediction model like LIME (where you can make hypothetical statements), problematic when features are correlated as sampling from the feature’s marginal distribution
Note: open research problem as of how to sample when features are correlated&lt;/p&gt;

&lt;p&gt;SHAP&lt;/p&gt;

&lt;p&gt;Example-based Explanations&lt;/p&gt;

&lt;p&gt;Counterfactual Explanations
Adversarial Examples
Prototypes and Criticisms
Influential Instances&lt;/p&gt;

&lt;p&gt;Resources:
[1] Interpretable Machine Learning Book (link)
[2] Towards a rigorous science of interpretable machine learning (link)
[3] Explaining explainability (link)
[4] Interpretable Machine Learning: the fuss, the concrete and the questions (link)&lt;br /&gt;
[5] Axiomatic Attribution for deep networks (link)
[6]A unified approach to interpreting model predictions (link)
[7] Predictive learning via rule ensembles (link)
[8] Captum: a model interpretability and understanding library for PyTorch (link)
[9] Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models (link)
[10] Why should I trust you?: Explaining the predictions of any classifier (link)&lt;/p&gt;
</description>
        <pubDate>Wed, 15 Jan 2020 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2020/machine-learning-interpretability/</link>
        <guid isPermaLink="true">http://localhost:4000//2020/machine-learning-interpretability/</guid>
        
        <category>data</category>
        
        <category>machine learning</category>
        
        <category>statistics</category>
        
        <category>interpretable ml</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>Interesting Research Topics</title>
        <description>&lt;p&gt;1.Explainability of Deep Learning&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Ranking problems&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2019/research_topicsmd/</link>
        <guid isPermaLink="true">http://localhost:4000//2019/research_topicsmd/</guid>
        
        <category>data</category>
        
        <category>machine learning</category>
        
        <category>statistics</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>Singular-value Decomposition</title>
        <description>&lt;p&gt;Singular-value Decomposition is widely used in collaborative filtering.&lt;br /&gt;
Imagine you are building a recommendation system for recommending books to users. You have m users and n potential books to recommend. You also have some historical ratings of some users on some books.&lt;br /&gt;
Based on this, you can formulate a utility matrix where each row represents a user and each column represents a book. You can, of course, calculate the correlation between users or books and recommend based on a correlation weighted ratings average, but the problem with that is the matrix is very sparse (you have lots of empty entries), and the process is very computationally expensive (hard to scale).&lt;br /&gt;
An idea to resolve this is to factorize the utility matrix using singular-value decomposition. By using, you can decompose the m&lt;em&gt;n matrix into three matrices: A = USV^T - m&lt;/em&gt;n = (m&lt;em&gt;k) * (k&lt;/em&gt;k) * (k*n). The idea is to do dimension reduction by extracting  latent factors from the utility matrix (and hopefully still preserves most variance). Intuitively, this is similar to finding a low dimensional manifold out of a high dimensional space. By doing so, you are able to represent information in a condenser way, and thus mitigate the data sparsity and computationally issue.&lt;br /&gt;
Following up on the above book recommendation example. Intuitively what we are doing here is finding k most ‘important’ features and project users and books onto the k-dimensional space these k features define. For example, one feature could be the genre of the book, another could be the language of the book. Both U and V are singular matrices. Each row of U represents an eigenvector for a user. Each column of V^T represents an eigenvector for a book. S is a diagonal matrix and each diagonal entry represents an eigenvalue (strength of the latent factor). The predicted rating from user i to book j will simply be a summation of the projection of i on dimension k, projection of j on dimension k, and the strength of dimension k over all k dimensions.&lt;br /&gt;
(Get the picture right, and SVD is easy to understand)&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jun 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2018/label_propagation/</link>
        <guid isPermaLink="true">http://localhost:4000//2018/label_propagation/</guid>
        
        <category>data</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>Label Propagation</title>
        <description>&lt;p&gt;Label propagation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;
Label propagation is a semi-supervised learning algorithm that assigns labels to previously unlabeled data points. It is a type of transductive learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Label Propagation on Graphs&lt;/strong&gt;
Only a subset of nodes in a graph is labeled. The task is to label  all nodes in a graph structure.  The underlying assumption is that linked nodes are correlated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Math&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Zoubin:
    &lt;ul&gt;
      &lt;li&gt;learning from labeled and unlabeled data with label propagation (paper) (http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf)&lt;/li&gt;
      &lt;li&gt;graph based semi-supervised learning (slides) (http://mlg.eng.cam.ac.uk/zoubin/talks/lect3ssl.pdf)&lt;/li&gt;
      &lt;li&gt;graph based semi-supervised learning (video) (https://www.youtube.com/watch?v=HZQOvm0fkLA)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zhukov, label propagation on graphs: slides (http://www.leonidzhukov.net/hse/2015/networks/lectures/lecture17.pdf), video1, (https://www.youtube.com/watch?v=hmashUPJwSQ&amp;amp;t=804s) video2 (https://www.youtube.com/watch?v=F4f247IyOTs&amp;amp;t=2699s)&lt;/li&gt;
  &lt;li&gt;semi-supervised learning literature survey (http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 15 Jun 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2018/SVD/</link>
        <guid isPermaLink="true">http://localhost:4000//2018/SVD/</guid>
        
        <category>data</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>2018读书计划和书单</title>
        <description>&lt;p&gt;2018：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Longitude-Genius-Greatest-Scientific-Problem/dp/080271529X&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The longitude&lt;/strong&gt;&lt;/a&gt;: 8/10&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Housekeeper-Professor-Yoko-Ogawa/dp/0312427808&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The housekeeper and the professor&lt;/strong&gt;&lt;/a&gt;: 7/10&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Then-There-Were-None/dp/0062073486&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;And then there were none&lt;/strong&gt;&lt;/a&gt;: 8/10&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Bit-Social-Research-Digital-Age/dp/0691158649&quot; target=&quot;_blank&quot;&gt;Bit by bit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Getting-Yes-Negotiating-Agreement-Without/dp/0140157352&quot; target=&quot;_blank&quot;&gt;Getting to yes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Idea-Factory-Great-American-Innovation/dp/0143122797&quot; target=&quot;_blank&quot;&gt;The idea factory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 15 Jun 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2018/book-list-2018/</link>
        <guid isPermaLink="true">http://localhost:4000//2018/book-list-2018/</guid>
        
        <category>books</category>
        
        
        <category>books</category>
        
      </item>
    
      <item>
        <title>30%与100%</title>
        <description>&lt;p&gt;对我来说，30%是很容易放弃的时候。&lt;br /&gt;
6mile的trail，往往一迈多的时候就不想爬了。工作也是：  &lt;br /&gt;
前20%是蜜月期，借着新鲜感和鸡血往往每天早上还能很high地满血复活；进入四五个月以后，就疲态略显。 
然而我也是度过了30%就越爬越high的人，70%－100%阶段的快感是最大的。&lt;br /&gt;
再想想自己的初心，再坚持一下，再努力一点。&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Mar 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2017/work-30_percent/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/work-30_percent/</guid>
        
        <category>misc</category>
        
        
        <category>misc</category>
        
      </item>
    
      <item>
        <title>追本溯源与单纯明快</title>
        <description>&lt;p&gt;做好一件事情就好了，所以要想的，是怎么突破自己的极限。&lt;br /&gt;
热情，动力，和掌控，都来源于对终极目标的明确；从此出发，去探索和延伸，从内而外，并以此目标判断日常是否值得去做，赋予日常以意义，才能做好，做的开心。&lt;br /&gt;
反之，则是被动和无穷的应付。&lt;br /&gt;
对于现在的工作，我该思考的唯一目标，就是：&lt;br /&gt;
怎么把model做到最好，并发挥最大的影响力？&lt;br /&gt;
从此出发，应该主动去探索model本身的可能性，学习，实验，测量，提高准确度；同时，怎样去推广，让别的组尽可能多的去用。&lt;br /&gt;
以此为尺，赋予日常以意义，才能真正的单纯明快。&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017/work/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/work/</guid>
        
        <category>misc</category>
        
        
        <category>misc</category>
        
      </item>
    
      <item>
        <title>2017我的旅行计划</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/blogs/magnets2017.jpg&quot; alt=&quot;magnets&quot; /&gt;
英国&amp;amp;法国&lt;br /&gt;
加拿大&lt;br /&gt;
南美：阿根廷/玻利维亚/秘鲁&lt;/p&gt;

</description>
        <pubDate>Fri, 13 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017/my-travel-plan/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/my-travel-plan/</guid>
        
        <category>travel</category>
        
        
        <category>travel</category>
        
      </item>
    
      <item>
        <title>2017读书计划和书单</title>
        <description>&lt;p&gt;2017， 10本：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://statweb.stanford.edu/~tibs/ElemStatLearn/&quot; target=&quot;_blank&quot;&gt;Elements of Statistical Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.otexts.org/fpp&quot; target=&quot;_blank&quot;&gt;Forecasting: principles and practice&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://db.ucsd.edu/static/TimeSeries.pdf&quot; target=&quot;_blank&quot;&gt;Time Series Analysis and Its Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/1458363/&quot; target=&quot;_blank&quot;&gt;The Razor’s Edge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/26427705/&quot; target=&quot;_blank&quot;&gt;三体3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/24753651/&quot; target=&quot;_blank&quot;&gt;Zero to One&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/3566719/&quot; target=&quot;_blank&quot;&gt;The professor and the housekeeper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/1813841/&quot; target=&quot;_blank&quot;&gt;Guns, Germs, and Steel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017/book-list-2017/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/book-list-2017/</guid>
        
        <category>books</category>
        
        
        <category>books</category>
        
      </item>
    
      <item>
        <title>A good introduction to random forest</title>
        <description>&lt;p&gt;Saw a pretty good introduction to random forest today:&lt;br /&gt;
http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/&lt;br /&gt;
—&lt;br /&gt;
Suppose you’re very indecisive, so whenever you want to watch a movie, you ask your friend Willow if she thinks you’ll like it. In order to answer, Willow first needs to figure out what movies you like, so you give her a bunch of movies and tell her whether you liked each one or not (i.e., you give her a labeled training set). Then, when you ask her if she thinks you’ll like movie X or not, she plays a 20 questions-like game with IMDB, asking questions like “Is X a romantic movie?”, “Does Johnny Depp star in X?”, and so on. She asks more informative questions first (i.e., she maximizes the information gain of each question), and gives you a yes/no answer at the end.&lt;/p&gt;

&lt;p&gt;Thus, Willow is a decision tree for your movie preferences.&lt;/p&gt;

&lt;p&gt;But Willow is only human, so she doesn’t always generalize your preferences very well (i.e., she overfits). In order to get more accurate recommendations, you’d like to ask a bunch of your friends, and watch movie X if most of them say they think you’ll like it. That is, instead of asking only Willow, you want to ask Woody, Apple, and Cartman as well, and they vote on whether you’ll like a movie (i.e., you build an ensemble classifier, aka a forest in this case).&lt;/p&gt;

&lt;p&gt;Now you don’t want each of your friends to do the same thing and give you the same answer, so you first give each of them slightly different data. After all, you’re not absolutely sure of your preferences yourself – you told Willow you loved Titanic, but maybe you were just happy that day because it was your birthday, so maybe some of your friends shouldn’t use the fact that you liked Titanic in making their recommendations. Or maybe you told her you loved Cinderella, but actually you really really loved it, so some of your friends should give Cinderella more weight. So instead of giving your friends the same data you gave Willow, you give them slightly perturbed versions. You don’t change your love/hate decisions, you just say you love/hate some movies a little more or less (formally, you give each of your friends a bootstrapped version of your original training data). For example, whereas you told Willow that you liked Black Swan and Harry Potter and disliked Avatar, you tell Woody that you liked Black Swan so much you watched it twice, you disliked Avatar, and don’t mention Harry Potter at all.&lt;/p&gt;

&lt;p&gt;By using this ensemble, you hope that while each of your friends gives somewhat idiosyncratic recommendations (Willow thinks you like vampire movies more than you do, Woody thinks you like Pixar movies, and Cartman thinks you just hate everything), the errors get canceled out in the majority. Thus, your friends now form a bagged (bootstrap aggregated) forest of your movie preferences.&lt;/p&gt;

&lt;p&gt;There’s still one problem with your data, however. While you loved both Titanic and Inception, it wasn’t because you like movies that star Leonardio DiCaprio. Maybe you liked both movies for other reasons. Thus, you don’t want your friends to all base their recommendations on whether Leo is in a movie or not. So when each friend asks IMDB a question, only a random subset of the possible questions is allowed (i.e., when you’re building a decision tree, at each node you use some randomness in selecting the attribute to split on, say by randomly selecting an attribute or by selecting an attribute from a random subset). This means your friends aren’t allowed to ask whether Leonardo DiCaprio is in the movie whenever they want. So whereas previously you injected randomness at the data level, by perturbing your movie preferences slightly, now you’re injecting randomness at the model level, by making your friends ask different questions at different times.&lt;/p&gt;

&lt;p&gt;And so your friends now form a random forest.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2016/introrf/</link>
        <guid isPermaLink="true">http://localhost:4000//2016/introrf/</guid>
        
        <category>data</category>
        
        
        <category>data</category>
        
      </item>
    
  </channel>
</rss>
