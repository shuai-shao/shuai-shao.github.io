<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shuai Shao</title>
    <description>单纯明快</description>
    <link>http://localhost:4000//</link>
    <atom:link href="http://localhost:4000//feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Thu, 02 Jan 2020 23:10:52 -0800</pubDate>
    <lastBuildDate>Thu, 02 Jan 2020 23:10:52 -0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Interesting Research Topics</title>
        <description>&lt;p&gt;1.Explainability of Deep Learning&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Ranking problems&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sun, 27 Oct 2019 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2019/research_topicsmd/</link>
        <guid isPermaLink="true">http://localhost:4000//2019/research_topicsmd/</guid>
        
        <category>data</category>
        
        <category>machine learning</category>
        
        <category>statistics</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>Singular-value Decomposition</title>
        <description>&lt;p&gt;Singular-value Decomposition is widely used in collaborative filtering.&lt;br /&gt;
Imagine you are building a recommendation system for recommending books to users. You have m users and n potential books to recommend. You also have some historical ratings of some users on some books.&lt;br /&gt;
Based on this, you can formulate a utility matrix where each row represents a user and each column represents a book. You can, of course, calculate the correlation between users or books and recommend based on a correlation weighted ratings average, but the problem with that is the matrix is very sparse (you have lots of empty entries), and the process is very computationally expensive (hard to scale).&lt;br /&gt;
An idea to resolve this is to factorize the utility matrix using singular-value decomposition. By using, you can decompose the m&lt;em&gt;n matrix into three matrices: A = USV^T - m&lt;/em&gt;n = (m&lt;em&gt;k) * (k&lt;/em&gt;k) * (k*n). The idea is to do dimension reduction by extracting  latent factors from the utility matrix (and hopefully still preserves most variance). Intuitively, this is similar to finding a low dimensional manifold out of a high dimensional space. By doing so, you are able to represent information in a condenser way, and thus mitigate the data sparsity and computationally issue.&lt;br /&gt;
Following up on the above book recommendation example. Intuitively what we are doing here is finding k most ‘important’ features and project users and books onto the k-dimensional space these k features define. For example, one feature could be the genre of the book, another could be the language of the book. Both U and V are singular matrices. Each row of U represents an eigenvector for a user. Each column of V^T represents an eigenvector for a book. S is a diagonal matrix and each diagonal entry represents an eigenvalue (strength of the latent factor). The predicted rating from user i to book j will simply be a summation of the projection of i on dimension k, projection of j on dimension k, and the strength of dimension k over all k dimensions.&lt;br /&gt;
(Get the picture right, and SVD is easy to understand)&lt;/p&gt;
</description>
        <pubDate>Mon, 25 Jun 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2018/label_propagation/</link>
        <guid isPermaLink="true">http://localhost:4000//2018/label_propagation/</guid>
        
        <category>data</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>Label Propagation</title>
        <description>&lt;p&gt;Label propagation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Definition&lt;/strong&gt;
Label propagation is a semi-supervised learning algorithm that assigns labels to previously unlabeled data points. It is a type of transductive learning.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Label Propagation on Graphs&lt;/strong&gt;
Only a subset of nodes in a graph is labeled. The task is to label  all nodes in a graph structure.  The underlying assumption is that linked nodes are correlated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Math&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;References&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Zoubin:
    &lt;ul&gt;
      &lt;li&gt;learning from labeled and unlabeled data with label propagation (paper) (http://mlg.eng.cam.ac.uk/zoubin/papers/CMU-CALD-02-107.pdf)&lt;/li&gt;
      &lt;li&gt;graph based semi-supervised learning (slides) (http://mlg.eng.cam.ac.uk/zoubin/talks/lect3ssl.pdf)&lt;/li&gt;
      &lt;li&gt;graph based semi-supervised learning (video) (https://www.youtube.com/watch?v=HZQOvm0fkLA)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Zhukov, label propagation on graphs: slides (http://www.leonidzhukov.net/hse/2015/networks/lectures/lecture17.pdf), video1, (https://www.youtube.com/watch?v=hmashUPJwSQ&amp;amp;t=804s) video2 (https://www.youtube.com/watch?v=F4f247IyOTs&amp;amp;t=2699s)&lt;/li&gt;
  &lt;li&gt;semi-supervised learning literature survey (http://pages.cs.wisc.edu/~jerryzhu/pub/ssl_survey.pdf)&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 15 Jun 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2018/SVD/</link>
        <guid isPermaLink="true">http://localhost:4000//2018/SVD/</guid>
        
        <category>data</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>2018读书计划和书单</title>
        <description>&lt;p&gt;2018：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Longitude-Genius-Greatest-Scientific-Problem/dp/080271529X&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The longitude&lt;/strong&gt;&lt;/a&gt;: 8/10&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Housekeeper-Professor-Yoko-Ogawa/dp/0312427808&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The housekeeper and the professor&lt;/strong&gt;&lt;/a&gt;: 7/10&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Then-There-Were-None/dp/0062073486&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;And then there were none&lt;/strong&gt;&lt;/a&gt;: 8/10&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Bit-Social-Research-Digital-Age/dp/0691158649&quot; target=&quot;_blank&quot;&gt;Bit by bit&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Getting-Yes-Negotiating-Agreement-Without/dp/0140157352&quot; target=&quot;_blank&quot;&gt;Getting to yes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.amazon.com/Idea-Factory-Great-American-Innovation/dp/0143122797&quot; target=&quot;_blank&quot;&gt;The idea factory&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 15 Jun 2018 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2018/book-list-2018/</link>
        <guid isPermaLink="true">http://localhost:4000//2018/book-list-2018/</guid>
        
        <category>books</category>
        
        
        <category>books</category>
        
      </item>
    
      <item>
        <title>30%与100%</title>
        <description>&lt;p&gt;对我来说，30%是很容易放弃的时候。&lt;br /&gt;
6mile的trail，往往一迈多的时候就不想爬了。工作也是：  &lt;br /&gt;
前20%是蜜月期，借着新鲜感和鸡血往往每天早上还能很high地满血复活；进入四五个月以后，就疲态略显。 
然而我也是度过了30%就越爬越high的人，70%－100%阶段的快感是最大的。&lt;br /&gt;
再想想自己的初心，再坚持一下，再努力一点。&lt;/p&gt;
</description>
        <pubDate>Wed, 29 Mar 2017 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2017/work-30_percent/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/work-30_percent/</guid>
        
        <category>misc</category>
        
        
        <category>misc</category>
        
      </item>
    
      <item>
        <title>追本溯源与单纯明快</title>
        <description>&lt;p&gt;做好一件事情就好了，所以要想的，是怎么突破自己的极限。&lt;br /&gt;
热情，动力，和掌控，都来源于对终极目标的明确；从此出发，去探索和延伸，从内而外，并以此目标判断日常是否值得去做，赋予日常以意义，才能做好，做的开心。&lt;br /&gt;
反之，则是被动和无穷的应付。&lt;br /&gt;
对于现在的工作，我该思考的唯一目标，就是：&lt;br /&gt;
怎么把model做到最好，并发挥最大的影响力？&lt;br /&gt;
从此出发，应该主动去探索model本身的可能性，学习，实验，测量，提高准确度；同时，怎样去推广，让别的组尽可能多的去用。&lt;br /&gt;
以此为尺，赋予日常以意义，才能真正的单纯明快。&lt;/p&gt;
</description>
        <pubDate>Thu, 26 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017/work/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/work/</guid>
        
        <category>misc</category>
        
        
        <category>misc</category>
        
      </item>
    
      <item>
        <title>2017我的旅行计划</title>
        <description>&lt;p&gt;&lt;img src=&quot;/images/blogs/magnets2017.jpg&quot; alt=&quot;magnets&quot; /&gt;
英国&amp;amp;法国&lt;br /&gt;
加拿大&lt;br /&gt;
南美：阿根廷/玻利维亚/秘鲁&lt;/p&gt;

</description>
        <pubDate>Fri, 13 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017/my-travel-plan/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/my-travel-plan/</guid>
        
        <category>travel</category>
        
        
        <category>travel</category>
        
      </item>
    
      <item>
        <title>2017读书计划和书单</title>
        <description>&lt;p&gt;2017， 10本：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://statweb.stanford.edu/~tibs/ElemStatLearn/&quot; target=&quot;_blank&quot;&gt;Elements of Statistical Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.otexts.org/fpp&quot; target=&quot;_blank&quot;&gt;Forecasting: principles and practice&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://db.ucsd.edu/static/TimeSeries.pdf&quot; target=&quot;_blank&quot;&gt;Time Series Analysis and Its Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/1458363/&quot; target=&quot;_blank&quot;&gt;The Razor’s Edge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/26427705/&quot; target=&quot;_blank&quot;&gt;三体3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/24753651/&quot; target=&quot;_blank&quot;&gt;Zero to One&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/3566719/&quot; target=&quot;_blank&quot;&gt;The professor and the housekeeper&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://book.douban.com/subject/1813841/&quot; target=&quot;_blank&quot;&gt;Guns, Germs, and Steel&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Wed, 04 Jan 2017 00:00:00 -0800</pubDate>
        <link>http://localhost:4000//2017/book-list-2017/</link>
        <guid isPermaLink="true">http://localhost:4000//2017/book-list-2017/</guid>
        
        <category>books</category>
        
        
        <category>books</category>
        
      </item>
    
      <item>
        <title>A good introduction to random forest</title>
        <description>&lt;p&gt;Saw a pretty good introduction to random forest today:&lt;br /&gt;
http://blog.echen.me/2011/03/14/laymans-introduction-to-random-forests/&lt;br /&gt;
—&lt;br /&gt;
Suppose you’re very indecisive, so whenever you want to watch a movie, you ask your friend Willow if she thinks you’ll like it. In order to answer, Willow first needs to figure out what movies you like, so you give her a bunch of movies and tell her whether you liked each one or not (i.e., you give her a labeled training set). Then, when you ask her if she thinks you’ll like movie X or not, she plays a 20 questions-like game with IMDB, asking questions like “Is X a romantic movie?”, “Does Johnny Depp star in X?”, and so on. She asks more informative questions first (i.e., she maximizes the information gain of each question), and gives you a yes/no answer at the end.&lt;/p&gt;

&lt;p&gt;Thus, Willow is a decision tree for your movie preferences.&lt;/p&gt;

&lt;p&gt;But Willow is only human, so she doesn’t always generalize your preferences very well (i.e., she overfits). In order to get more accurate recommendations, you’d like to ask a bunch of your friends, and watch movie X if most of them say they think you’ll like it. That is, instead of asking only Willow, you want to ask Woody, Apple, and Cartman as well, and they vote on whether you’ll like a movie (i.e., you build an ensemble classifier, aka a forest in this case).&lt;/p&gt;

&lt;p&gt;Now you don’t want each of your friends to do the same thing and give you the same answer, so you first give each of them slightly different data. After all, you’re not absolutely sure of your preferences yourself – you told Willow you loved Titanic, but maybe you were just happy that day because it was your birthday, so maybe some of your friends shouldn’t use the fact that you liked Titanic in making their recommendations. Or maybe you told her you loved Cinderella, but actually you really really loved it, so some of your friends should give Cinderella more weight. So instead of giving your friends the same data you gave Willow, you give them slightly perturbed versions. You don’t change your love/hate decisions, you just say you love/hate some movies a little more or less (formally, you give each of your friends a bootstrapped version of your original training data). For example, whereas you told Willow that you liked Black Swan and Harry Potter and disliked Avatar, you tell Woody that you liked Black Swan so much you watched it twice, you disliked Avatar, and don’t mention Harry Potter at all.&lt;/p&gt;

&lt;p&gt;By using this ensemble, you hope that while each of your friends gives somewhat idiosyncratic recommendations (Willow thinks you like vampire movies more than you do, Woody thinks you like Pixar movies, and Cartman thinks you just hate everything), the errors get canceled out in the majority. Thus, your friends now form a bagged (bootstrap aggregated) forest of your movie preferences.&lt;/p&gt;

&lt;p&gt;There’s still one problem with your data, however. While you loved both Titanic and Inception, it wasn’t because you like movies that star Leonardio DiCaprio. Maybe you liked both movies for other reasons. Thus, you don’t want your friends to all base their recommendations on whether Leo is in a movie or not. So when each friend asks IMDB a question, only a random subset of the possible questions is allowed (i.e., when you’re building a decision tree, at each node you use some randomness in selecting the attribute to split on, say by randomly selecting an attribute or by selecting an attribute from a random subset). This means your friends aren’t allowed to ask whether Leonardo DiCaprio is in the movie whenever they want. So whereas previously you injected randomness at the data level, by perturbing your movie preferences slightly, now you’re injecting randomness at the model level, by making your friends ask different questions at different times.&lt;/p&gt;

&lt;p&gt;And so your friends now form a random forest.&lt;/p&gt;
</description>
        <pubDate>Thu, 22 Sep 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2016/introrf/</link>
        <guid isPermaLink="true">http://localhost:4000//2016/introrf/</guid>
        
        <category>data</category>
        
        
        <category>data</category>
        
      </item>
    
      <item>
        <title>突然想起老头子</title>
        <description>&lt;p&gt;用我妈的话说，老爸是个老顽童。  &lt;br /&gt;
从小带我打游戏机，看足球，下军旗的都是老爸。&lt;br /&gt;
还有背诗。老爸记忆力很好，年轻的时候也是个文艺小青年。我小的时候，经常问我古文学到哪了，然后即兴给我背首诗：&lt;br /&gt;
卖炭翁，伐薪烧炭南山中。。。&lt;br /&gt;
八月秋高风怒号，卷我屋上三重茅。。。&lt;br /&gt;
老头子普通话又不标准，甚是有意思。&lt;br /&gt;
今天突然想到汶川地震后，老爸拉着老妈，无论如何要领养一个受灾的孩子，最终因为政策不允许只能作罢。&lt;br /&gt;
经常给老妈打电话，回头该给老头子打个电话了。&lt;/p&gt;
</description>
        <pubDate>Mon, 29 Aug 2016 00:00:00 -0700</pubDate>
        <link>http://localhost:4000//2016/dad/</link>
        <guid isPermaLink="true">http://localhost:4000//2016/dad/</guid>
        
        <category>misc</category>
        
        
        <category>misc</category>
        
      </item>
    
  </channel>
</rss>
